{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compliance Data Work\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import os\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statistics import mean, median, mode, stdev \n",
    "\n",
    "# from Descript import extract\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../DATA/2807.csv\", sep=\",\", header=0)\n",
    "\n",
    "# Convert Date attributes from type object to datetime object.\n",
    "df['TreatmentStartDate'] = pd.to_datetime(df['TreatmentStartDate'], format='%Y-%m-%d %H:%M')\n",
    "df['CtrlDatetime'] = pd.to_datetime(df['CtrlDatetime'], format='%Y-%m-%d %H:%M')\n",
    "df['installDate'] = pd.to_datetime(df['installDate'], format='%Y-%m-%d')\n",
    "\n",
    "# Set new attributes of Time and Date of Treatment\n",
    "df['treatmentDate'] = df['TreatmentStartDate'].dt.date\n",
    "# df['treatmentDate'] = pd.to_datetime(df['treatmentDate'])\n",
    "\n",
    "df['treatmentTime'] = df['TreatmentStartDate'].dt.time\n",
    "# df['treatmentTime'] = pd.to_datetime(df['treatmentTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data) or \n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df[['CutitronicsClientID','CutitronicsSKUName','CutitronicsCartID', 'installDate', 'TreatmentStartDate', 'treatmentDate', 'treatmentTime', 'CartDispensedAmount','CartLevel', 'CartInitLevel', 'presrcibedAM', 'prescribedPM','PrescriptionType', 'CtrlSkinHealth']]\n",
    "\n",
    "for i, row in dfa.iterrows():\n",
    "    if row['treatmentTime'].hour < 4:\n",
    "        am_val = 0\n",
    "        pm_val = dfa.loc[i, 'CartDispensedAmount']\n",
    "    elif row['treatmentTime'].hour > 17:\n",
    "        am_val = 0\n",
    "        pm_val = dfa.loc[i, 'CartDispensedAmount']\n",
    "    else:\n",
    "        am_val = dfa.loc[i, 'CartDispensedAmount']\n",
    "        pm_val = 0\n",
    "    dfa.loc[i,'actualAM'] = am_val\n",
    "    dfa.loc[i,'actualPM'] = pm_val\n",
    "\n",
    "dfa[['actualAM', 'actualPM', 'CtrlSkinHealth']] = dfa[['actualAM', 'actualPM', 'CtrlSkinHealth']].astype('int64')\n",
    "\n",
    "dfa = dfa.groupby(['CutitronicsClientID', 'CutitronicsSKUName', 'treatmentDate']).agg({ \n",
    "    'installDate': 'last',\n",
    "    'CutitronicsCartID': 'first',\n",
    "    'CartDispensedAmount': 'sum',\n",
    "    'CartLevel': 'last',\n",
    "    'CartInitLevel': 'first',\n",
    "    'presrcibedAM':'first',\n",
    "    'prescribedPM':'last',\n",
    "    'actualAM': 'sum',\n",
    "    'actualPM':'sum',\n",
    "    'PrescriptionType': 'first',\n",
    "    'CtrlSkinHealth': 'mean'\n",
    "})\n",
    "\n",
    "for i, row in dfa.iterrows():\n",
    "    if row['PrescriptionType'] == 'DAILY':\n",
    "        if row['presrcibedAM'] == row['actualAM'] and row['prescribedPM'] == row['actualPM']:\n",
    "            complied = 'Yes' # 1 = True (User Complied with prescription for that date)\n",
    "        else:\n",
    "            complied = 'No' # 0 = False (User did not comply with prescription for that date)\n",
    "        dfa.loc[i,'compliance'] = complied\n",
    "        \n",
    "    else:\n",
    "        if row['presrcibedAM'] == row['actualAM'] or row['prescribedPM'] == row['actualPM']:\n",
    "            complied = 'Yes' # 1 = True (User Complied with prescription for that date)\n",
    "        else:\n",
    "            complied = 'No' # 0 = False (User did not comply with prescription for that date)\n",
    "        dfa.loc[i,'compliance'] = complied\n",
    "    \n",
    "dfa[['actualAM', 'actualPM']] = dfa[['actualAM', 'actualPM']].astype('int64')\n",
    "\n",
    "a = dfa['compliance'].value_counts(normalize=True) * 100\n",
    "labels = dfa['compliance'].value_counts().index.tolist()\n",
    "print(a)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.bar(labels, a, color='green')\n",
    "plt.xlabel(labels)\n",
    "plt.ylabel(\"Percentage of Users\")\n",
    "plt.title(\"Percentage of Users Complying with Prescription\")\n",
    "plt.show()\n",
    "\n",
    "asR = dfa.loc[dfa['PrescriptionType'] == 'AS REQUIRED'].reset_index()\n",
    "df = dfa.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missed(dfName):\n",
    "    # Assign global to returned DFs\n",
    "    global df\n",
    "    global grouped_df\n",
    "    \n",
    "    # Assign df to the df name provided\n",
    "    df = dfName\n",
    "    \n",
    "    # Apply funcion used for finding missed usage dates\n",
    "    def foo(gr):\n",
    "        gr = gr.set_index('treatmentDate')\n",
    "        idx = pd.date_range(gr.index.min(), gr.index.max())\n",
    "        gr.index = pd.DatetimeIndex(gr.index)\n",
    "        gr = gr.reindex(idx, fill_value=0)\n",
    "        return gr\n",
    "    \n",
    "    # Groupby and Apply function\n",
    "    df = df[df['PrescriptionType'] == 'DAILY'].groupby(['CutitronicsClientID', 'CutitronicsSKUName']).apply(func=foo)\n",
    "    \n",
    "    # Groupy causes duplicate column with one as index\n",
    "    # The col version is dropped to allow for reset of index\n",
    "    df = df.drop(['CutitronicsClientID', 'CutitronicsSKUName'], axis = 1) \n",
    "    grouped_df = df\n",
    "    df = df.reset_index()\n",
    "    df =df.rename(columns={\"level_2\": \"treatmentDate\"})\n",
    "    \n",
    "    # Default of 0 is applied to all vlaues in missed usage row.\n",
    "    # Alters comliance value to 'Missed' rather than 0\n",
    "    for i, row in df.iterrows():\n",
    "            if row['compliance'] == 0:\n",
    "                complied = 'MISSED'\n",
    "                date = row['treatmentDate']\n",
    "            else:\n",
    "                complied = row['compliance']\n",
    "            df.loc[i,'compliance'] = complied\n",
    "        \n",
    "# Call function with df created previously\n",
    "missed(df)\n",
    "\n",
    "# # Join DF's back together\n",
    "main_df = pd.concat([df, asR], join='inner').reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['CutitronicsClientID','CutitronicsSKUName'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = main_df.groupby(['CutitronicsClientID', 'CutitronicsSKUName'])\n",
    "print(a.head())\n",
    "for ID, data in main_df.groupby(['CutitronicsClientID', 'CutitronicsSKUName']):\n",
    "    for i, row in data.iterrows():\n",
    "        if i == 0:\n",
    "            a = data.loc[i, 'CartInitLevel'] - data.loc[i, 'CartLevel']\n",
    "            data.loc[i, 'productUsed'] = a\n",
    "        else:\n",
    "            a = data.loc[i-1, 'CartLevel'] - data.loc[i, 'CartLevel']\n",
    "            data.loc[i, 'productUsed'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = main_df.loc[main_df['CutitronicsClientID'] == 'client5']\n",
    "test.head()\n",
    "for i, row in test.iterrows():\n",
    "            print(i)\n",
    "            if i == 0:\n",
    "                print(\"here\")\n",
    "                a = df.loc[i, 'CartInitLevel'] - df.loc[i, 'CartLevel']\n",
    "                print(a)\n",
    "            else:\n",
    "                a = df.loc[i-1, 'CartLevel'] - df.loc[i, 'CartLevel']\n",
    "                print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in main_df.iterrows():\n",
    "#     d1 = row['treatmentDate']\n",
    "#     d0 = row['installDate']\n",
    "    \n",
    "#     delta = d1-d0\n",
    "#     main_df.loc[i, 'daysInstalled'] = delta\n",
    "\n",
    "# d1 = df['treatmentDate'][3]\n",
    "# d0 = df['installDate'][3]\n",
    "# delta = d1 - d0\n",
    "# print(delta.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['compliance'].value_counts(normalize=True) * 100\n",
    "labels = df['compliance'].value_counts().index.tolist()\n",
    "print(values)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel(labels)\n",
    "plt.ylabel(\"Percentage of Users\")\n",
    "plt.title(\"Percentage of Users Complying with Prescription\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby(['CutitronicsSKUName']).plot(x='treatmentDate', y='CartLevel', style='o')  \n",
    "plt.title('Dispensed Amount vs CartLevel')  \n",
    "plt.xlabel('Dispensed Amount')  \n",
    "plt.ylabel('Cart Level')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ID, data in main_df.groupby(['CutitronicsClientID', 'CutitronicsSKUName']):\n",
    "    values = data['compliance'].value_counts(normalize=True) * 100\n",
    "    labels = data['compliance'].value_counts().index.tolist()\n",
    "    \n",
    "    plt.pie(values, labels=labels,\n",
    "        autopct='%5.0f%%', shadow=True)\n",
    "    \n",
    "    #draw a circle at the center of pie to make it look like a donut\n",
    "    centre_circle = plt.Circle((0,0),0.75,color='black', fc='white',linewidth=1)\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    \n",
    "    # Set aspect ratio to be equal so that pie is drawn as a circle.\n",
    "    plt.axis('equal')\n",
    "    plt.title('{} Compliance \\n'.format(ID))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ID, data in main_df.groupby(['CutitronicsClientID', 'CutitronicsSKUName']):\n",
    "    data.index = data['treatmentDate']\n",
    "    plt.title(\"Skin Health Rating for {}\".format(ID))\n",
    "    #plot\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(data['CtrlSkinHealth'], label='Skin Health history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = main_df['compliance'].value_counts(normalize=True) * 100\n",
    "labels = main_df['compliance'].value_counts().index.tolist()\n",
    "\n",
    "colors = ['yellow', 'green', 'red']\n",
    "explode = (0, 0, 0, 0)  # explode a slice if required\n",
    "\n",
    "plt.pie(values, labels=labels,\n",
    "        autopct='%1.1f%%', shadow=True)\n",
    "        \n",
    "#draw a circle at the center of pie to make it look like a donut\n",
    "centre_circle = plt.Circle((0,0),0.75,color='black', fc='white',linewidth=1)\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "\n",
    "# Set aspect ratio to be equal so that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "plt.title('Compliance to Prescription \\n')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MP_KEY = 'pk.eyJ1IjoiYW5keWZhcnJlbGw3IiwiYSI6ImNrZDV6bmh2bTA2MzQyc241YXdoOWRkY2sifQ.Jt6edR8CCaS88tGae3m12A'\n",
    "a = pd.read_csv(\"../DATA/2807.csv\", sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# fig = px.scatter_mapbox(a, lat=\"clientLat\", lon=\"clientLon\", color=\"Area\", hover_name='Area', hover_data=['CtrlSkinHealth'], zoom=4)\n",
    "# fig.update_layout(mapbox_style=\"light\", mapbox_accesstoken=MP_KEY)\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# values = main_df['compliance'].value_counts(normalize=True) * 100\n",
    "# labels = main_df['compliance'].value_counts().index.tolist()\n",
    "\n",
    "# print(values)\n",
    "\n",
    "# # Use `hole` to create a donut-like pie chart\n",
    "# fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5)])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ID, data in main_df.groupby(['CutitronicsClientID']):\n",
    "    values = data['compliance'].value_counts(normalize=True) * 100\n",
    "    labels = data['compliance'].value_counts().index.tolist()\n",
    "    \n",
    "    # Use `hole` to create a donut-like pie chart\n",
    "    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6, pull=[0, 0, 0.1])])\n",
    "    fig.update_layout(title=\"{}'s Compliance\".format(ID))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compPercent(series):\n",
    "    a = series.value_counts()\n",
    "    b = sum(a)\n",
    "    if 'Yes' in a:\n",
    "        return round(a['Yes'] / b * 100, 2)\n",
    "    else:\n",
    "        return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = main_df.groupby(['CutitronicsClientID', 'treatmentDate']).agg({ \n",
    "    'CtrlSkinHealth': 'mean',\n",
    "    'compliance': compPercent\n",
    "})\n",
    "\n",
    "new_df = new_df.reset_index()\n",
    "\n",
    "fig = px.line(new_df, x=\"treatmentDate\", y=\"CtrlSkinHealth\", color='CutitronicsClientID', hover_data=['compliance'])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "# MACHINE LEARNING  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = new_df.loc[new_df['CutitronicsClientID'] == 'client1'].groupby(['treatmentDate']).agg({ \n",
    "    'CtrlSkinHealth': 'mean'\n",
    "})\n",
    "df3 = df3.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollingHealth = df3['CtrlSkinHealth'].rolling(5).mean()\n",
    "\n",
    "fig = px.line(df3, x=\"treatmentDate\", y=rollingHealth)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.set_index('treatmentDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4.loc['2020-06-19', 'CtrlSkinHealth'] = 74\n",
    "# df4.loc['2020-06-20', 'CtrlSkinHealth'] = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.plot(style='k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(7).mean()\n",
    "    rolstd = timeseries.rolling(7).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #perform dickey fuller test  \n",
    "    print(\"Results of dickey fuller test\")\n",
    "    adft = adfuller(timeseries['CtrlSkinHealth'],autolag='AIC')\n",
    "    # output for dft will give us without defining what the values are.\n",
    "    #hence we manually write what values does it explains using a for loop\n",
    "    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n",
    "    for key,values in adft[4].items():\n",
    "        output['critical value (%s)'%key] =  values\n",
    "    print(output)\n",
    "    \n",
    "test_stationarity(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = np.log(df4)\n",
    "moving_avg = df_log.rolling(7).mean()\n",
    "std_dev = df_log.rolling(7).std()\n",
    "plt.plot(df_log)\n",
    "plt.plot(moving_avg, color=\"red\")\n",
    "plt.plot(std_dev, color =\"black\")\n",
    "plt.show()\n",
    "\n",
    "df_log_moving_avg_diff = df_log-moving_avg\n",
    "df_log_moving_avg_diff.dropna(inplace=True)\n",
    "\n",
    "test_stationarity(df_log_moving_avg_diff)\n",
    "\n",
    "weighted_average = df_log.ewm(halflife=7, min_periods=0,adjust=True).mean()\n",
    "\n",
    "logScale_weightedMean = df_log-weighted_average\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10,6\n",
    "logScale_weightedMean.dropna(inplace=True)\n",
    "test_stationarity(logScale_weightedMean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "# we use d value here(data_log_shift)\n",
    "acf = acf(df_log, nlags=5)\n",
    "pacf= pacf(df_log, nlags=5,method='ols')#plot PACF\n",
    "plt.subplot(121)\n",
    "plt.plot(acf) \n",
    "plt.axhline(y=0,linestyle='-',color='blue')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(logScale_weightedMean)),linestyle='--',color='black')\n",
    "plt.axhline(y=1.96/np.sqrt(len(logScale_weightedMean)),linestyle='--',color='black')\n",
    "plt.title('Auto corellation function')\n",
    "plt.tight_layout()#plot ACF\n",
    "plt.subplot(122)\n",
    "plt.plot(pacf) \n",
    "plt.axhline(y=0,linestyle='-',color='blue')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(logScale_weightedMean)),linestyle='--',color='black')\n",
    "plt.axhline(y=1.96/np.sqrt(len(logScale_weightedMean)),linestyle='--',color='black')\n",
    "plt.title('Partially auto corellation function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_diff = df_log - df_log.shift()\n",
    "plt.title(\"Shifted timeseries\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "plt.plot(df_log_diff)#Let us test the stationarity of our resultant series\n",
    "df_log_diff.dropna(inplace=True)\n",
    "test_stationarity(df_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(df_log, order=(7,1,5))\n",
    "result_AR = model.fit(disp = 0)\n",
    "plt.plot(df_log_diff)\n",
    "plt.plot(result_AR.fittedvalues, color='green')\n",
    "plt.title(\"sum of squares of residuals\")\n",
    "print('RSS : %f' %sum((result_AR.fittedvalues-df_log_diff[\"CtrlSkinHealth\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_AR.plot_predict(1,65)\n",
    "x=result_AR.forecast(steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statsmodels\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('finalDF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.loc[new_df['CutitronicsClientId'] == 'client5']\n",
    "\n",
    "df5 = new_df.loc[new_df['CutitronicsClientID'] == 'client1'].groupby(['treatmentDate']).agg({ \n",
    "# df5 = new_df.groupby(['treatmentDate']).agg({ \n",
    "    'CtrlSkinHealth': 'mean',\n",
    "    'compliance': 'mean'\n",
    "})\n",
    "df5 = df5.reset_index()\n",
    "df5 = df5.set_index('treatmentDate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df5 = df5[['CtrlSkinHealth','compliance']]\n",
    "df5['CtrlSkinHealth'] = df5['CtrlSkinHealth'].astype('float32')\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, dpi=120, figsize=(10,6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    data = df5[df5.columns[i]]\n",
    "    ax.plot(data, color='red', linewidth=1)\n",
    "    # Decorations\n",
    "    ax.set_title(df5.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=12\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "#     \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "#     The rows are the response variable, columns are predictors. The values in the table \n",
    "#     are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "#     the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "#     zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "#     data      : pandas dataframe containing the time series variables\n",
    "#     variables : list containing names of the time series variables.\n",
    "#     \"\"\"\n",
    "    df5 = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df5.columns:\n",
    "        for r in df5.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df5.loc[r, c] = min_p_value\n",
    "    df5.columns = [var + '_x' for var in variables]\n",
    "    df5.index = [var + '_y' for var in variables]\n",
    "    print(\"Granger Causality Results \\n\\n\", df5)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "grangers_causation_matrix(df5, variables = df5.columns)\n",
    "\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df5, alpha=0.05): \n",
    "#     \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df5,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print(\"Cointegration Test \\n\\n\" + 'Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df5.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(df5)\n",
    "\n",
    "\n",
    "nobs = 10\n",
    "df5_train, df5_test = df5[0:-nobs], df5[-nobs:]\n",
    "\n",
    "# Check size\n",
    "print(\"\\n\\n\", df5_train.shape)  # (53, 2)\n",
    "print(df5_test.shape, \"\\n\\n\")  # (4, 2)\n",
    "\n",
    "\n",
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key,val in r[4].items():\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")\n",
    "        \n",
    "# ADF Test on each column\n",
    "for name, column in df5_train.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')\n",
    "\n",
    "# df_differenced = df5_train\n",
    "# First Difference\n",
    "df_differenced = df5_train.diff().dropna()\n",
    "# ADF Test on each column of 1st Differences Dataframe\n",
    "for name, column in df_differenced.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')\n",
    "\n",
    "# # Second Difference\n",
    "# df_differenced = df_differenced.diff().dropna()\n",
    "# # ADF Test on each column of 1st Differences Dataframe\n",
    "# for name, column in df_differenced.iteritems():\n",
    "#     adfuller_test(column, name=column.name)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "model = VAR(df_differenced)\n",
    "for i in [1,2,3,4,5,6,7,8,9]:\n",
    "    result = model.fit(i)\n",
    "    print('Lag Order =', i)\n",
    "    print('AIC : ', result.aic)\n",
    "    print('BIC : ', result.bic)\n",
    "    print('FPE : ', result.fpe)\n",
    "    print('HQIC: ', result.hqic, '\\n')\n",
    "\n",
    "x = model.select_order(maxlags=12)\n",
    "print(x.summary())\n",
    "\n",
    "model_fitted = model.fit(8)\n",
    "print(model_fitted.summary())\n",
    "\n",
    "# Get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order)  #> 4\n",
    "\n",
    "# Input data for forecasting\n",
    "forecast_input = df_differenced.values[-lag_order:]\n",
    "print(\"forecast Input\", forecast_input)\n",
    "\n",
    "# Forecast\n",
    "fc = model_fitted.forecast(y=forecast_input, steps=nobs)\n",
    "df_forecast = pd.DataFrame(fc, index=df5.index[-nobs:], columns=df5.columns + '_2d')\n",
    "df_forecast = df_forecast.astype('int32')\n",
    "print(df_forecast.head(15))\n",
    "\n",
    "def invert_transformation(df_train, df_forecast, second_diff=False):\n",
    "    \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n",
    "    df_fc = df_forecast.copy()\n",
    "    columns = df5_train.columns\n",
    "    for col in columns:        \n",
    "        # Roll back 2nd Diff\n",
    "        if second_diff:\n",
    "            df_fc[str(col)+'_1d'] = (df5_train[col].iloc[-1]-df5_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n",
    "       # Roll back 1st Diff\n",
    "        df_fc[str(col)+'_forecast'] = df5_train[col].iloc[-1] + df_fc[str(col)+'_2d'].cumsum()\n",
    "    return df_fc\n",
    "\n",
    "df_results = invert_transformation(df5_train, df_forecast, second_diff=False)  \n",
    "print(df_results[['CtrlSkinHealth_forecast']])\n",
    "# df_results.loc[:, ['CtrlSkinHealth', 'compliance']]\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=1, dpi=120, figsize=(10,6))\n",
    "# for i, (col,ax) in enumerate(zip(df5.columns, axes.flatten())):\n",
    "#     df_forecast[col+'_2d'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "#     df5_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "#     ax.set_title(col + \": Forecast vs Actuals\")\n",
    "#     ax.xaxis.set_ticks_position('none')\n",
    "#     ax.yaxis.set_ticks_position('none')\n",
    "#     ax.spines[\"top\"].set_alpha(0)\n",
    "#     ax.tick_params(labelsize=6)\n",
    "\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "for col, val in zip(df5.columns, out):\n",
    "    print(\"\\n\\n Durbin Watson Statistic:\",adjust(col), ':', round(val, 2))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, dpi=150, figsize=(10,10))\n",
    "for i, (col,ax) in enumerate(zip(df5.columns, axes.flatten())):\n",
    "    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    df5_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})\n",
    "\n",
    "print('Forecast Accuracy of: compliance')\n",
    "accuracy_prod = forecast_accuracy(df_results['compliance_forecast'].values, df5_test['compliance'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,8))\n",
    "\n",
    "print('\\nForecast Accuracy of: CtrlSkinHealth')\n",
    "accuracy_prod = forecast_accuracy(df_results['CtrlSkinHealth_forecast'].values, df5_test['CtrlSkinHealth'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, dpi=120, figsize=(10,6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    data = df5[df5.columns[i]]\n",
    "    ax.plot(data, color='red', linewidth=1)\n",
    "    # Decorations\n",
    "    ax.set_title(df5.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=10\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "#     \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "#     The rows are the response variable, columns are predictors. The values in the table \n",
    "#     are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "#     the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "#     zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "#     data      : pandas dataframe containing the time series variables\n",
    "#     variables : list containing names of the time series variables.\n",
    "#     \"\"\"\n",
    "    df5 = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df5.columns:\n",
    "        for r in df5.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df5.loc[r, c] = min_p_value\n",
    "    df5.columns = [var + '_x' for var in variables]\n",
    "    df5.index = [var + '_y' for var in variables]\n",
    "    return df5\n",
    "\n",
    "grangers_causation_matrix(df5, variables = df5.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df5, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df5,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df5.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 5\n",
    "df5_train, df5_test = df5[0:-nobs], df5[-nobs:]\n",
    "\n",
    "# Check size\n",
    "print(df5_train.shape)  # (53, 2)\n",
    "print(df5_test.shape)  # (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key,val in r[4].items():\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test on each column\n",
    "for name, column in df5_train.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(df5_train)\n",
    "for i in [1,2,3,4,5,6,7,8,9]:\n",
    "    result = model.fit(i)\n",
    "    print('Lag Order =', i)\n",
    "    print('AIC : ', result.aic)\n",
    "    print('BIC : ', result.bic)\n",
    "    print('FPE : ', result.fpe)\n",
    "    print('HQIC: ', result.hqic, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.select_order(maxlags=9)\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted = model.fit(8)\n",
    "model_fitted.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order)  #> 4\n",
    "\n",
    "# Input data for forecasting\n",
    "forecast_input = df5_train.values[-lag_order:]\n",
    "forecast_input\n",
    "\n",
    "# Forecast\n",
    "fc = model_fitted.forecast(y=forecast_input, steps=nobs)\n",
    "df_forecast = pd.DataFrame(fc, index=df5.index[-nobs:], columns=df5.columns + '_2d')\n",
    "df_forecast = df_forecast.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, dpi=120, figsize=(10,6))\n",
    "for i, (col,ax) in enumerate(zip(df5.columns, axes.flatten())):\n",
    "    df_forecast[col+'_2d'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    df5_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's take a look at my dataframe\n",
    "\n",
    "# fig, ax = plt.subplots(df5.shape[1], 1, figsize=(10, 10))\n",
    "# plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# for i in range(df5.shape[1]):\n",
    "#     color = 'r' if i == 0 else 'b'\n",
    "#     ax[i].plot(df[ df5.columns[i] ], color, label = df5.columns[i])\n",
    "#     ax[i].legend(prop={'size': 8})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1 Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# #2 Importing the dataset\n",
    "X = df5.iloc[:,0:1].values.astype(float)\n",
    "y = df5.iloc[:,1:2].values.astype(float)\n",
    "\n",
    "\n",
    "#3 Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)\n",
    "\n",
    "\n",
    "#4 Fitting the Support Vector Regression Model to the dataset\n",
    "# Create your support vector regressor here\n",
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "regressor = SVR(kernel='linear')\n",
    "regressor.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svr = SVR(kernel='linear').fit(X, y)\n",
    "print(svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = svr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(X, yfit, lw=2, color=\"red\", label=\"fitted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "score = svr.score(X,y)\n",
    "print(\"R-squared:\", score)\n",
    "print(\"MSE:\", mean_squared_error(y, yfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df5.index\n",
    "X = pd.to_numeric(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
